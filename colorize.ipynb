{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "The syntax of the command is incorrect.\n",
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!mkdir data/raw\n",
    "!mkdir data/processed\n",
    "!mkdir data/processed/unsplash\n",
    "!wget https://unsplash.com/data/lite/latest -O data/raw/unsplash.zip\n",
    "!unzip data/raw/unsplash -d data/raw/unsplash\n",
    "!rm data/raw/unsplash.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "import skimage.color as color\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "input_dir = Path(\"data\") / \"raw\" / \"unsplash\"\n",
    "output_dir = Path(\"data\") / \"processed\" / \"unsplash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the images\n",
    "df = pd.read_csv(input_dir / \"photos.tsv000\", sep='\\t', header=0)\n",
    "\n",
    "def remove_transparency(im, bg_colour=(255, 255, 255)):\n",
    "    # Only process if image has transparency (http://stackoverflow.com/a/1963146)\n",
    "    if im.mode in ('RGBA', 'LA') or (im.mode == 'P' and 'transparency' in im.info):\n",
    "        background = Image.new(\"RGB\", im.size, (255, 255, 255))\n",
    "        background.paste(im, mask = im.split()[3])\n",
    "        return background\n",
    "    else:\n",
    "        return im\n",
    "    \n",
    "def download_and_process(i):\n",
    "    try:\n",
    "        url = df['photo_image_url'][i]\n",
    "        photo_id = df['photo_id'][i]\n",
    "        im = Image.open(requests.get(url, stream=True, timeout=2).raw)\n",
    "        im = remove_transparency(im)\n",
    "        im.thumbnail((1000, 1000), Image.ANTIALIAS)\n",
    "        im.save(output_dir / (photo_id + '.jpg'), optimize=True)\n",
    "    except:\n",
    "        failed.append(i)\n",
    "\n",
    "failed = []\n",
    "for i in tqdm(df.index):\n",
    "    download_and_process(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/processed/unsplash/train\n",
    "!mkdir data/processed/unsplash/test\n",
    "!mkdir data/processed/unsplash/val\n",
    "\n",
    "\n",
    "all_images = list(Path(output_dir).glob(\"*.jpg\"))\n",
    "for i, img_p in enumerate(all_images):\n",
    "    if i < 15000:\n",
    "        os.rename(img_p, output_dir / \"train\" / img_p.name)\n",
    "    elif i < 20000:\n",
    "        os.rename(img_p, output_dir / \"test\" / img_p.name)\n",
    "    else:\n",
    "        os.rename(img_p, output_dir / \"val\" / img_p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizeDataset(Dataset):    \n",
    "    def __init__(self, image_path, transform=None):\n",
    "        self.image_names = list(Path(image_path).glob(\"*.jpg\"))\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_names[idx]\n",
    "        im = Image.open(image_name)\n",
    "        # discard alpha channel\n",
    "        im = im.convert('RGB')\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        im = np.asarray(im) / 255\n",
    "        lab = color.rgb2lab(im)\n",
    "        lab = lab.transpose((2, 0, 1))\n",
    "        lab = torch.as_tensor(lab, dtype=torch.float32)\n",
    "        \n",
    "        l_channel = torch.unsqueeze(lab[0], 0) / 100\n",
    "        ab_channels = (lab[1:] + 127) / 255\n",
    "        return l_channel, ab_channels\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_lab(x, y):\n",
    "        return np.concatenate([x * 100, y * 255 - 127], axis=0).transpose((1, 2, 0))\n",
    "    \n",
    "    @staticmethod\n",
    "    def lab2rgb(lab):\n",
    "        return color.lab2rgb(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = ColorizeDataset(output_dir / 'train')\n",
    "fig1, axes1 = plt.subplots(nrows=5, ncols=5, figsize=(15,15))\n",
    "fig2, axes2 = plt.subplots(nrows=5, ncols=5, figsize=(15,15))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        x, y = example_dataset[i * 5 + j]\n",
    "        axes1[i][j].imshow(x[0], cmap='Greys')\n",
    "        lab = ColorizeDataset.create_lab(x, y)\n",
    "        axes2[i][j].imshow(ColorizeDataset.lab2rgb(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "            ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(224)\n",
    "            ])\n",
    "        }\n",
    "        # split dataset\n",
    "        if stage == 'fit':\n",
    "            self.train, self.val = ColorizeDataset(output_dir / 'train', data_transforms['train']), ColorizeDataset(output_dir / 'val', data_transforms['val'])\n",
    "        if stage == 'test':\n",
    "            self.test = ColorizeDataset(output_dir / 'test', data_transforms['test'])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, self.batch_size)\n",
    "    \n",
    "data = ColorizeDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # replace first layer to take in single-channel inputs\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, 7, 2, 3, bias=False)\n",
    "        \n",
    "        self.features = nn.Sequential(*list(resnet.children())[0:5]) # 256 x 56 x 56\n",
    "        \n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3), # 128 x 54 x 54\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2), # 128 x 108 x 108\n",
    "            nn.Conv2d(128, 64, 3), # 64 x 106 x 106\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample((226, 226)), # 64 x 226 x 226\n",
    "            nn.Conv2d(64, 2, 3), # 2 x 224 x 224\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert(x.dim() in [3, 4])\n",
    "        assert(x.shape[-1] == 224 and x.shape[-2] == 224)\n",
    "        if x.dim() == 3:\n",
    "            x = x.reshape(x.shape[0], 1, 224, 224)\n",
    "        else: # x.dim() == 4\n",
    "            assert(x.shape[1] == 1)\n",
    "        \n",
    "        features = self.features(x)\n",
    "        return self.upsample(features)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        # logging to TensorBoard\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        # logging to TensorBoard\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y_true = batch\n",
    "        y_pred = self(x)\n",
    "        loss = F.mse_loss(y_pred, y_true)\n",
    "        # logging to TensorBoard\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2, 224, 224])\n",
      "tensor(0.1858)\n",
      "tensor(0.9899)\n"
     ]
    }
   ],
   "source": [
    "model = ResnetAutoEncoder()\n",
    "x = torch.zeros(16, 1, 224, 224)\n",
    "y = model(x)\n",
    "print(y.shape)\n",
    "print(y.min())\n",
    "print(y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexd\\miniconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | features | Sequential | 219 K \n",
      "1 | upsample | Sequential | 370 K \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]torch.Size([32, 2, 224, 224])\n",
      "torch.Size([32, 2, 224, 224])\n",
      "Validation sanity check:  50%|█████     | 1/2 [00:03<00:03,  3.52s/it]torch.Size([32, 2, 224, 224])\n",
      "torch.Size([32, 2, 224, 224])\n",
      "Epoch 0:   0%|          | 3/625 [00:23<1:19:54,  7.71s/it, loss=0.019, v_num=6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexd\\miniconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "trainer = pl.trainer.Trainer(callbacks=[\n",
    "    EarlyStopping(monitor='val_loss')\n",
    "], max_epochs=10)\n",
    "\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
